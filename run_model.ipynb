{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bbea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./Final_Model/working/final_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "from transformers import LogitsProcessorList, NoBadWordsLogitsProcessor\n",
    "\n",
    "bad_words = [[t] for t in tokenizer.encode(\"A:\", add_special_tokens=False)]\n",
    "processor = LogitsProcessorList([NoBadWordsLogitsProcessor(bad_words_ids=bad_words)])\n",
    "\n",
    "\n",
    "\n",
    "def generate(user_input):\n",
    "    prompt = f\"Q: {user_input}\\nA:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = input_ids[\"input_ids\"].shape[1]  # Length of the prompt\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=500,\n",
    "            \n",
    "            # min_new_tokens=25,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            no_repeat_ngram_size=10,    # reduce repetition\n",
    "            logits_processor=processor,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    # Slice off the prompt part\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "while(True):\n",
    "    user_input=input(\"How can i help you: \")\n",
    "    if(user_input==\"bye\" or user_input==\"quit\"):\n",
    "        break\n",
    "\n",
    "    response=generate(user_input)\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
